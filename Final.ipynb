{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Local\\Continuum\\Anaconda2\\envs\\TensorFlow\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        data = []\n",
    "        data_temp = []\n",
    "        labels = []\n",
    "        labels_temp = []\n",
    "\n",
    "        for line in f.read().splitlines():\n",
    "            if line != '':  \n",
    "                a = line.split('\\t')\n",
    "                data_temp.append(a[0])\n",
    "                labels_temp.append(a[1])\n",
    "            else:\n",
    "                data.append(data_temp)\n",
    "                labels.append(labels_temp)\n",
    "                data_temp = []\n",
    "                labels_temp = []\n",
    "\n",
    "    f.close()\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_no_labels(test_file_name):\n",
    "    with open(test_file_name, encoding='utf-8') as f:\n",
    "        data = []\n",
    "        data_temp = []\n",
    "\n",
    "        for line in f.read().splitlines():\n",
    "            if line != '':  \n",
    "                data_temp.append(line)\n",
    "            else:\n",
    "                data.append(data_temp)\n",
    "                data_temp = []\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = read_file('data/train/train.txt')\n",
    "dev_data, dev_labels = read_file('data/dev/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = read_no_labels('data/test/test.nolabels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read external sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/train/eng.list\", encoding='utf-8') as f:\n",
    "    eng_data = []\n",
    "    eng_labels = []\n",
    "\n",
    "    for line in f.read().splitlines():\n",
    "        if line != '':  \n",
    "            temp = line.split(' ')\n",
    "            eng_data.append(temp[1:])\n",
    "            eng_labels.append(['B'] + ['I'] * (len(temp)-2))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_names():\n",
    "    name_data = set([])\n",
    "\n",
    "    def read_names_files(filename):\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            #f.read() # Get rid of the header\n",
    "            for line in f.read().splitlines():\n",
    "                if line != '':\n",
    "                    temp = line.split(',')\n",
    "                    name_data.add(temp[0].strip(' \\t\\n\\r').split(' ')[0])\n",
    "                    name_data.add(temp[1].strip(' \\t\\n\\r').split(' ')[0])\n",
    "        f.close()\n",
    "\n",
    "    read_names_files(\"male-names.csv\")\n",
    "    read_names_files(\"female-names.csv\")\n",
    "    return name_data\n",
    "\n",
    "name_data = read_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vec(word):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return ['NULL']*300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS & Chunk tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*>} # Chunk verbs and their arguments\n",
    "  \"\"\"\n",
    "cp=nltk.RegexpParser(grammar)\n",
    "\n",
    "def chunck_tag(sentence):\n",
    "    tree = cp.parse(sentence)\n",
    "    return nltk.chunk.tree2conlltags(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. During development we only use the training file as data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = [chunck_tag(nltk.pos_tag(train_data[i])) for i in range(len(train_data))]\n",
    "dev_data = [chunck_tag(nltk.pos_tag(dev_data[i])) for i in range(len(dev_data))]\n",
    "test_data = [chunck_tag(nltk.pos_tag(test_data[i])) for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. For submitting, we use both training and dev files as data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train_data = train_data + dev_data\n",
    "new_train_labels = train_labels + dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train_data = [chunck_tag(nltk.pos_tag(new_train_data[i])) for i in range(len(new_train_data))]\n",
    "test_data = [chunck_tag(nltk.pos_tag(test_data[i])) for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for sent in train_data:\n",
    "#     for word in sent:\n",
    "#         if word[1] == 'NNPS':\n",
    "#             count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_words = [item[0] for item in eng_data]\n",
    "middle_words = [item for sublist in eng_data for item in sublist[1:]]\n",
    "\n",
    "lower_init_words = [item[0].lower() for item in eng_data]\n",
    "lower_middle_words = [item.lower() for sublist in eng_data for item in sublist[1:]]\n",
    "name_data = read_names()\n",
    "\n",
    "def Word2Features(sentence, pos):\n",
    "    features = {}\n",
    "    features.update(current_word_features(sentence[pos][0]))\n",
    "    features.update(w2vfeatuers(sentence[pos][0]))\n",
    "    if pos > 0:\n",
    "        features.update(prev_word_features(sentence[pos-1][0]))\n",
    "#         features.update(w2vfeatuers(sentence[pos-1][0], suffix=\"prev_\"))\n",
    "        features.update(external_sources_features(sentence,pos-1, suffix=\"prev_\"))\n",
    "    else:\n",
    "        features.update(begin_of_sentence())\n",
    "\n",
    "    if pos < len(sentence)-1:\n",
    "        features.update(next_word_features(sentence[pos+1][0]))\n",
    "#         features.update(w2vfeatuers(sentence[pos+1][0], suffix=\"next_\"))\n",
    "        features.update(external_sources_features(sentence,pos+1, suffix=\"next_\"))\n",
    "    else:\n",
    "        features.update(end_of_sentence())\n",
    "\n",
    "    features.update(external_sources_features(sentence,pos))\n",
    "    features.update(tag_features(sentence,pos))\n",
    "    features.update(chunk_features(sentence,pos))\n",
    "    return features\n",
    "\n",
    "\n",
    "def w2vfeatuers(word, suffix=\"\"):\n",
    "    w2vfeatures = {}\n",
    "    for index, letter in enumerate(get_word_vec(word)):\n",
    "        w2vfeatures.update({suffix + 'wv_value'+str(index): letter})\n",
    "    return w2vfeatures\n",
    "\n",
    "\n",
    "def current_word_features(word):\n",
    "    return {\n",
    "        'bias': 1.0,\n",
    "        'lower': word.lower(),\n",
    "        'suffix_4': word[-4:],\n",
    "        'suffix_3': word[-3:],\n",
    "        'suffix_2': word[-2:],\n",
    "        'isupper': word.isupper() * 25.0,\n",
    "        'istitle': word.istitle() * 5.0,\n",
    "        'isdigit': word.isdigit(),\n",
    "#         'has_digit': True if re.match(r'.*[0-9].*', word) else False,\n",
    "#         'single_digit': True if re.match(r'[0-9]', word) else False,\n",
    "#         'double_digit': True if re.match(r'[0-9][0-9]', word) else False,\n",
    "#         'has_dash': True if re.match(r'.*-.*', word) else False,\n",
    "        'punct': True if re.match(r'[.,;:?!-+\\'\"]', word) else False,\n",
    "        'istwittertag': isTwitterTag(word)\n",
    "    }\n",
    "\n",
    "def isTwitterTag(word):\n",
    "    return word[0] == '@' or word[0] == '#'\n",
    "\n",
    "\n",
    "def prev_word_features(word):\n",
    "    return {\n",
    "        'prev_lower': word.lower(),\n",
    "        'prev_istitle': word.istitle(),\n",
    "        'prev_isupper': word.isupper(),\n",
    "    }\n",
    "\n",
    "\n",
    "def next_word_features(word):\n",
    "    return {\n",
    "        'next_lower': word.lower(),\n",
    "        'next_istitle': word.istitle(),\n",
    "        'next_isupper': word.isupper(),\n",
    "    }\n",
    "\n",
    "\n",
    "def tag_features(sentence, pos):\n",
    "    pos_features = {'pos[0]': sentence[pos][1]}\n",
    "    prev_prev_pos_tag = sentence[pos-2][1] if pos > 1 else 'START'\n",
    "    prev_pos_tag = sentence[pos-1][1] if pos > 0 else 'START'\n",
    "    next_pos_tag = sentence[pos+1][1] if pos < len(sentence)-1 else 'END'\n",
    "    next_next_pos_tag = sentence[pos+2][1] if pos < len(sentence)-2 else 'END'\n",
    "    pos_features.update({\n",
    "#         'pos[-2]': prev_prev_pos_tag,\n",
    "#         'pos[-1]': prev_pos_tag,\n",
    "#         'pos[+1]': next_pos_tag,\n",
    "#         'pos[+2]': next_next_pos_tag,\n",
    "        'pos[-2]|pos[-1]': prev_prev_pos_tag + '|' + prev_pos_tag,\n",
    "        'pos[-1]|pos[0]': prev_pos_tag + '|' + sentence[pos][1],\n",
    "        'pos[0]|pos[+1]': sentence[pos][1] + '|' + next_pos_tag,\n",
    "        'pos[+1]|pos[+2]': next_pos_tag + '|' + next_next_pos_tag,\n",
    "        'pos[-2]|pos[-1]|pos[0]': prev_prev_pos_tag + '|' + prev_pos_tag + '|' + sentence[pos][1],\n",
    "        'pos[-1]|pos[0]|pos[+1]': prev_pos_tag + '|' + sentence[pos][1] + '|' + next_pos_tag,\n",
    "        'pos[0]|pos[+1]|pos[+2]': sentence[pos][1] + '|' + next_pos_tag + '|' + next_next_pos_tag,\n",
    "    })\n",
    "    return pos_features\n",
    "\n",
    "\n",
    "def chunk_features(sentence, pos):\n",
    "    chunk_features = {'chunk[0]': sentence[pos][2]}\n",
    "    prev_prev_chunk_tag = sentence[pos-2][2] if pos > 1 else 'NULL'\n",
    "    prev_chunk_tag = sentence[pos-1][2] if pos > 0 else 'NULL'\n",
    "    next_chunk_tag = sentence[pos+1][2] if pos < len(sentence)-1 else 'NULL'\n",
    "    next_next_chunk_tag = sentence[pos+2][2] if pos < len(sentence)-2 else 'NULL'\n",
    "    chunk_features.update({\n",
    "#         'chunk[-1]': prev_chunk_tag,\n",
    "#         'chunk[+1]': next_chunk_tag,\n",
    "        'chunk[-2]|chunk[-1]': prev_prev_chunk_tag + '|' + prev_chunk_tag,\n",
    "        'chunk[-1]|chunk[0]': prev_chunk_tag + '|' + sentence[pos][2],\n",
    "        'chunk[0]|chunk[+1]': sentence[pos][2] + '|' + next_chunk_tag,\n",
    "        'chunk[+1]|chunk[+2]': next_chunk_tag + '|' + next_next_chunk_tag,\n",
    "#         'chunk[-2]|chunk[-1]|chunk[0]': prev_prev_chunk_tag + '|' + prev_chunk_tag + '|' + sentence[pos][2],\n",
    "#         'chunk[-1]|chunk[0]|chunk[+1]': prev_chunk_tag + '|' + sentence[pos][2] + '|' + next_chunk_tag,\n",
    "#         'chunk[0]|chunk[+1]|chunk[+2]': sentence[pos][2] + '|' + next_chunk_tag + '|' + next_next_chunk_tag,\n",
    "\n",
    "    })\n",
    "    return chunk_features\n",
    "\n",
    "\n",
    "def begin_of_sentence():\n",
    "    return {'BOS': True}\n",
    "\n",
    "\n",
    "def end_of_sentence():\n",
    "    return {'EOS': True}\n",
    "\n",
    "\n",
    "def external_sources_features(sentence, pos, suffix=\"\"):\n",
    "    return {\n",
    "        suffix + 'begin': is_begin_of_external(sentence[pos][0]),\n",
    "        suffix + 'middle': is_middle_of_external(sentence[pos][0]),\n",
    "        suffix + 'beginT': is_title_begin_of_external(sentence[pos][0]),\n",
    "        suffix + 'middleT': is_title_middle_of_external(sentence[pos][0]),\n",
    "        suffix + 'beginL': is_lower_begin_of_external(sentence[pos][0]),\n",
    "        suffix + 'middleL': is_lower_middle_of_external(sentence[pos][0]),\n",
    "        suffix + 'both': is_both_of_external(sentence[pos][0]),\n",
    "        suffix + 'name': is_external_name(sentence[pos][0]),\n",
    "    }\n",
    "\n",
    "def is_begin_of_external(word):\n",
    "    return word in init_words\n",
    "\n",
    "def is_middle_of_external(word):\n",
    "    return word in middle_words\n",
    "\n",
    "def is_lower_begin_of_external(word):\n",
    "    return word.lower() in lower_init_words\n",
    "\n",
    "def is_lower_middle_of_external(word):\n",
    "    return word.lower() in lower_middle_words\n",
    "\n",
    "def is_title_begin_of_external(word):\n",
    "    return (word.title() in init_words) * 4.0\n",
    "\n",
    "def is_title_middle_of_external(word):\n",
    "    return (word.title() in middle_words) * 4.0\n",
    "\n",
    "def is_both_of_external(word):\n",
    "    return is_begin_of_external(word) and is_middle_of_external(word)\n",
    "\n",
    "def is_external_name(word):\n",
    "    return (word.lower() in name_data) * 15.0\n",
    "\n",
    "def is_any_external(word):\n",
    "    return is_lower_begin_of_external(word) or is_lower_middle_of_external(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = [[Word2Features(s, pos) for pos in range(len(s))] for s in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Run just on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [[Word2Features(s, pos) for pos in range(len(s))] for s in train_data]\n",
    "y_train = train_labels\n",
    "\n",
    "X_dev = [[Word2Features(s, pos) for pos in range(len(s))] for s in dev_data]\n",
    "y_dev = dev_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Run in both the training and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [[Word2Features(s, pos) for pos in range(len(s)) ] for s in new_train_data]\n",
    "y_train = new_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf_CV = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': [0.5, 0.6, 0.65, 0.7, 0.75, 0.8],\n",
    "    'c2': [0.5, 0.6, 0.65, 0.7, 0.75, 0.8],\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = GridSearchCV(crf_CV, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"This are the results of all training\")\n",
    "for score in rs.grid_scores_:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "crf_final = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.5, \n",
    "    c2=0.5, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = list(crf_final.classes_)\n",
    "labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_pred = crf_final.predict(X_train)\n",
    "metrics.flat_f1_score(y_train, y_train_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_dev_pred = crf_final.predict(X_dev)\n",
    "metrics.flat_f1_score(y_dev, y_dev_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test_pred = crf_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.flat_classification_report(\n",
    "    y_dev, y_dev_pred, labels=labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(dev_data):\n",
    "    if y_dev_pred[i] != dev_labels[i]:\n",
    "        print('\\n')\n",
    "        print(sentence)\n",
    "    for j in range(len(sentence)):\n",
    "        if y_dev_pred[i][j] != dev_labels[i][j]:\n",
    "            print(str(sentence[j]) + \"is \" + str(dev_labels[i][j]) + \", but we said \" + str(y_dev_pred[i][j]) + \\\n",
    "                  \"Name \" + str(is_external_name(sentence[j][0])) + \"External:\" + str(is_any_external(sentence[j][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_output(pred, outputfile):\n",
    "    f = open(outputfile,'w')\n",
    "    for label_sentence in pred:\n",
    "        for label_word in label_sentence:\n",
    "            f.write(label_word + '\\n')\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_output(y_train_pred, \"output-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_output(y_dev_pred, \"output-dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_output(y_test_pred, \"output-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf_final.state_features_).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf_final.state_features_).most_common()[-50:])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [TensorFlow]",
   "language": "python",
   "name": "Python [TensorFlow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
